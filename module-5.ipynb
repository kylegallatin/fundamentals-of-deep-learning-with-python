{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Deep learning for natural language processing\n",
    "\n",
    "## Overview\n",
    "At its core, Natural Language Processing is about enabling computers to understand, interpret, and produce human language in a way that is both meaningful and valuable. This interdisciplinary domain sits at the intersection of computer science, artificial intelligence, and linguistics. The applications of NLP are vast and varied, spanning from simple tasks like spell-checkers and search engines to more complex ones like chatbots, sentiment analysis, and machine translation.\n",
    "\n",
    "## Deep learning in NLP\n",
    "The last decade has witnessed a transformative shift in the way we model language, thanks to the advent of deep learning. Traditional NLP techniques often relied on handcrafted features and rule-based approaches. However, deep learning, with its ability to automatically learn representations from data, has provided a more powerful and scalable way to tackle intricate NLP challenges.\n",
    "\n",
    "## Goals of this notebook\n",
    "- gain familiarity working with text in Python and representing it in ways that work for neural networks\n",
    "- initialize and train a neural network to create word embeddings using PyTorch\n",
    "- train a neural network for text classification using a transformer model and PyTorch \n",
    "\n",
    "## Non-goals\n",
    "- understand _all_ deep learning and text processing code in this notebook at a deeper level\n",
    "- understand the underlying architecture of the pre-trained transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Neural Networks\n",
    "Before feeding text data into a neural network, it is crucial to preprocess and convert it into a format that the model can understand. The aim of text preprocessing is to clean and standardize the text data, making it free of any inconsistencies and irrelevant elements. Let's explore the fundamental steps involved in this process.\n",
    "\n",
    "Tokenization: This is the process of breaking down text into smaller units, typically words or tokens. For example, the sentence \"I love Anaconda!\" can be tokenized into the words [\"I\", \"love\", \"Anaconda\", \"!\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love Anaconda!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercasing: Often, text data comes in a mix of upper and lower case. Since \"Anaconda\" and \"anaconda\" are treated as different tokens, converting all tokens to lowercase helps in reducing the size of our vocabulary and ensuring consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords: These are commonly used words in a language (like \"and\", \"is\", \"in\") which might not contribute significantly to the meaning of a sentence in certain NLP tasks. Removing them can often help reduce the dimensionality of the data.\n",
    "\n",
    "Punctuation: Punctuation marks can often be extraneous in NLP tasks. Removing them further cleans the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming: This method reduces words to their root form, even if the root is not a valid word. For instance, \"running\" would be stemmed to \"runn\".\n",
    "\n",
    "Lemmatization: It reduces words to their base or dictionary form. For instance, \"running\" would be lemmatized to \"run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokens = [\"I\", \"love\", \"anacondas\"]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding: Each word in the vocabulary is represented by a vector where one element is \"hot\" or 1, indicating the presence of the word, while all other elements are zero.\n",
    "\n",
    "Word embeddings: A dense representation of words in a continuous vector space where semantically similar words are positioned closer together. While there are pre-trained embeddings like Word2Vec and GloVe, we'll be training our embeddings in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming a predefined vocabulary from our dataset.\n",
    "vocab = {\"I\": 0, \"love\": 1, \"Anaconda\": 2}  # This is just a small example; in practice, it'll be larger.\n",
    "\n",
    "\n",
    "tokens = [\"I\", \"love\", \"Anaconda\", \"!\"]\n",
    "\n",
    "token_ids = [vocab[token] for token in tokens if token in vocab]\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings using PyTorch\n",
    "Word embeddings transform words into dense vectors in a continuous space, capturing semantic relationships between words based on their co-occurrence patterns. For instance, in the embedding space, the vectors for words like \"king\" and \"queen\" might be closer together due to their similar contexts and meanings.\n",
    "\n",
    "Word embeddings represent a major leap from traditional sparse representations like one-hot encoding. While a one-hot encoded vector has dimensions equal to the size of the vocabulary and is mostly filled with zeros, a word embedding is a dense vector with a much smaller dimension, e.g., 100 or 300, and captures semantic meaning.\n",
    "\n",
    "Some popular pre-trained word embeddings are Word2Vec, GloVe, and FastText. They are trained on massive corpora and can be readily used. However, for domain-specific tasks or languages with less resources, it's often beneficial to train your own embeddings.\n",
    "\n",
    "## Training a Simple Word Embedding Model with PyTorch\n",
    "In PyTorch, the Embedding layer is designed to handle word embeddings. It initializes a matrix of `[vocab_size x embedding_dim]` where each row corresponds to a vector representation of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = len(vocab)  # Assuming you've defined a vocab from your dataset previously.\n",
    "embedding_dim = 100  # Typical dimensions are 50, 100, 200, 300, etc.\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the embeddings for specific words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our previous vocab example\n",
    "word_ids = torch.tensor([vocab[\"I\"], vocab[\"Anaconda\"]], dtype=torch.long)\n",
    "word_embeddings = embedding_layer(word_ids)\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a very simple example of creating an embedding layer and accessing it. But what about training embeddings ourselves? Let's use the sample `AG_NEWS` dataset in PyTorch to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "# Tokenize the AG_NEWS dataset\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "counter = Counter()\n",
    "for (label, line) in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter)\n",
    "\n",
    "# Convert token to tensor\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "\n",
    "# Example\n",
    "text_pipeline('I love Anaconda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view a few samples from the datsaset to see what we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "i = 0\n",
    "for (label, line) in train_iter:\n",
    "    if i < 5:\n",
    "        print(f\"label: {label}, line: {line}\")\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a lightweight class to manage our data prior to passing in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, text, window_size=4):\n",
    "        self.window_size = window_size\n",
    "        self.data = []\n",
    "        for i, center_word in enumerate(text):\n",
    "            for j in range(i - window_size, i + window_size + 1):\n",
    "                if j != i and j >= 0 and j < len(text):\n",
    "                    self.data.append((center_word, text[j]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Example dataset usage\n",
    "toy_text = text_pipeline('I love Anaconda')\n",
    "skipgram_dataset = SkipGramDataset(toy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one more class that will represent our skip-gram PyTorch model that let's us initialize and train word embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, center_words):\n",
    "        embeds = self.embeddings(center_words)\n",
    "        out = self.linear(embeds)\n",
    "        log_probabilities = self.log_softmax(out)\n",
    "        return log_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally - we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "\n",
    "model = SkipGram(len(vocab), embedding_dim)\n",
    "criterion = nn.NLLLoss() # Negative Log Likelihood Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    dataloader = DataLoader(skipgram_dataset, batch_size=32, shuffle=True)\n",
    "    for center_word, target_word in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(center_word)\n",
    "        loss = criterion(log_probs, target_word)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this training process, model.embeddings contains the \"learned\" word embeddings. The small amount of trianing epochs we used here might not yield very useful word embeddings, however.\n",
    "\n",
    "This approach uses a simplified skip-gram model for clarity and brevity. In practice, models like Word2Vec employ optimizations such as negative sampling to make training more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification using Transformer Layer in PyTorch\n",
    "Transformers, introduced in the paper \"Attention is All You Need\" by Vaswani et al., revolutionized the NLP landscape by presenting a new way to handle sequential data without relying on recurrence. At the heart of the transformer architecture is the attention mechanism, which allows the model to focus on different parts of the input text.\n",
    "\n",
    "In this section, we'll employ a simple transformer layer to classify text.\n",
    "\n",
    "## Setting up the dataset\n",
    "\n",
    "AG News is a dataset for news classification. The goal is to classify a news article into one of 4 classes: World, Sports, Business, Science/Technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "batch_size = 8\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256)\n",
    " \n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_data = tokenized_datasets[\"train\"].select(range(1000))\n",
    "test_data = tokenized_datasets[\"test\"].select(range(10))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model and optimizer\n",
    "We'll use a pretrained BERT model for classification as our model. Since AG News has 4 categories, we'll be setting the num_labels parameter to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define the optimizer and scheduler we'll use to train this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "We'll train our model for a few epochs. For the sake of this demonstration, we'll set it to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, working with transformers can be resource-intensive, especially with longer sequences.\n",
    "\n",
    "## Evaluating the model\n",
    "Last step is evaluating the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
