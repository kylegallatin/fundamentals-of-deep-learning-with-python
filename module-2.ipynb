{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Working with Gradients in PyTorch\n",
    "\n",
    "Last unit we talked about some of the operations that occur during a forward pass in a neural network. However, a backward pass involves a different set of important mathematical operations - particularly those that enable the neural network to \"learn\". \n",
    "\n",
    "In particular, a backward pass involves calculating the gradient of the loss function with respect to the weights of the network using a technique called _gradient descent_. The gradients are calculated layer by layer, starting from the output layer and moving backward towards the input layer. The gradients are then used to update the weights of the network through an optimization algorithm which iteratively adjusts the weights in the direction that minimizes the loss function.\n",
    "\n",
    "Minimizing the loss function updates the neural network in a manner that iterally minimizes error on it's chosen task, and is what we commonly call \"learning\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd in PyTorch\n",
    "\n",
    "Autograd is one of the core features of PyTorch and a big factor in its popularity as a deep learning library. Autograd makes it easy to compute and store gradients - which makes PyTorch very intuitive and flexible for researchers and practitioners familiar with deep learning operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a torch tensor with `requires_grad=True`\n",
    "t = torch.tensor([1.0, 5.0, 10.0], requires_grad=True)\n",
    "\n",
    "# A miscellaneous operation simulating \"forward propagation\"\n",
    "tensor_sum = t.sum()\n",
    "\n",
    "# Perform back propagation and view gradients\n",
    "tensor_sum.backward()\n",
    "t.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch stores these gradients using a computational directed acyclic graph (DAG). When working with gradients, we need to be mindul of the operations we perform - as some operations aren't won't be supported using PyTorch autograd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a5d98a1e3ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert the tensor to a NumPy array, we can see that we need to use the `detach` method. This is because autograd isn't supported by NumPy - and therefore any computations we perform on the tensor once converted to a NumPy array can't be tracked by autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  5., 10.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Neural Network Classes in PyTorch\n",
    "\n",
    "So far, we've just done toy examples with small tensor operations - but we haven't used one of the most prominent patterns in PyTorch: using `nn.Module`.\n",
    "\n",
    "The `nn.Module` class in PyTorch implements a number of useful methods and attributes for us behind the scenes, allowing us to focus on the core components used to build our neural network, and the deep learning process itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass Output:\n",
      "tensor([[1.1551],\n",
      "        [1.6131],\n",
      "        [2.0110]], grad_fn=<AddmmBackward0>)\n",
      "Gradients:\n",
      "tensor([[ 0.0729,  0.2701],\n",
      "        [-0.0900,  0.2890],\n",
      "        [-0.0900,  0.2890]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3)  # Fully connected layer with input size 2 and output size 3\n",
    "        self.fc2 = nn.Linear(3, 1)  # Fully connected layer with input size 3 and output size 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define some input data\n",
    "input_data = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], requires_grad=True)\n",
    "\n",
    "# Perform a forward pass\n",
    "output = model.forward(input_data)\n",
    "\n",
    "# Print the output\n",
    "print(\"Forward Pass Output:\")\n",
    "print(output)\n",
    "\n",
    "# Perform a backward pass (compute gradients)\n",
    "output.backward(torch.ones_like(output))\n",
    "\n",
    "# Print the gradients\n",
    "print(\"Gradients:\")\n",
    "print(input_data.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above constitutes _one iteration_ of model training, where we conducted 1 forward pass and 1 backward pass. In a real scenario, we'd conduct many forward and backward pass to slowly update the network weights and cause it to learn on our data.\n",
    "\n",
    "In fact, let's update the sample above to do that right now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 0.7083749771118164\n",
      "Epoch 11/500, Loss: 0.2700563967227936\n",
      "Epoch 21/500, Loss: 0.237418994307518\n",
      "Epoch 31/500, Loss: 0.2305573970079422\n",
      "Epoch 41/500, Loss: 0.2284841686487198\n",
      "Epoch 51/500, Loss: 0.22763872146606445\n",
      "Epoch 61/500, Loss: 0.2271665334701538\n",
      "Epoch 71/500, Loss: 0.22682584822177887\n",
      "Epoch 81/500, Loss: 0.226541206240654\n",
      "Epoch 91/500, Loss: 0.22631622850894928\n",
      "Epoch 101/500, Loss: 0.22612178325653076\n",
      "Epoch 111/500, Loss: 0.2259395271539688\n",
      "Epoch 121/500, Loss: 0.22576852142810822\n",
      "Epoch 131/500, Loss: 0.22560793161392212\n",
      "Epoch 141/500, Loss: 0.2254570722579956\n",
      "Epoch 151/500, Loss: 0.22531533241271973\n",
      "Epoch 161/500, Loss: 0.2251824289560318\n",
      "Epoch 171/500, Loss: 0.22505740821361542\n",
      "Epoch 181/500, Loss: 0.2249399870634079\n",
      "Epoch 191/500, Loss: 0.22482885420322418\n",
      "Epoch 201/500, Loss: 0.22471646964550018\n",
      "Epoch 211/500, Loss: 0.22461330890655518\n",
      "Epoch 221/500, Loss: 0.22452150285243988\n",
      "Epoch 231/500, Loss: 0.22443516552448273\n",
      "Epoch 241/500, Loss: 0.22434568405151367\n",
      "Epoch 251/500, Loss: 0.2242639809846878\n",
      "Epoch 261/500, Loss: 0.22419176995754242\n",
      "Epoch 271/500, Loss: 0.22411155700683594\n",
      "Epoch 281/500, Loss: 0.22404752671718597\n",
      "Epoch 291/500, Loss: 0.22397565841674805\n",
      "Epoch 301/500, Loss: 0.22391855716705322\n",
      "Epoch 311/500, Loss: 0.22385382652282715\n",
      "Epoch 321/500, Loss: 0.2238023281097412\n",
      "Epoch 331/500, Loss: 0.22374427318572998\n",
      "Epoch 341/500, Loss: 0.2236890345811844\n",
      "Epoch 351/500, Loss: 0.22364391386508942\n",
      "Epoch 361/500, Loss: 0.22359563410282135\n",
      "Epoch 371/500, Loss: 0.2235480695962906\n",
      "Epoch 381/500, Loss: 0.22350282967090607\n",
      "Epoch 391/500, Loss: 0.2234598845243454\n",
      "Epoch 401/500, Loss: 0.22342292964458466\n",
      "Epoch 411/500, Loss: 0.22338713705539703\n",
      "Epoch 421/500, Loss: 0.22335012257099152\n",
      "Epoch 431/500, Loss: 0.22331444919109344\n",
      "Epoch 441/500, Loss: 0.22328050434589386\n",
      "Epoch 451/500, Loss: 0.22324800491333008\n",
      "Epoch 461/500, Loss: 0.22321699559688568\n",
      "Epoch 471/500, Loss: 0.22318631410598755\n",
      "Epoch 481/500, Loss: 0.22315478324890137\n",
      "Epoch 491/500, Loss: 0.22312617301940918\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3)  # Fully connected layer with input size 2 and output size 3\n",
    "        self.fc2 = nn.Linear(3, 1)  # Fully connected layer with input size 3 and output size 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define some input data and corresponding target labels (ground truth)\n",
    "input_data = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], requires_grad=True)\n",
    "target_labels = torch.tensor([[0.0], [1.0], [0.0]])\n",
    "\n",
    "# Define a loss function (Mean Squared Error, MSE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer (Stochastic Gradient Descent, SGD) to update the model's parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    # Zero the gradients to prevent accumulation\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Perform a forward pass\n",
    "    output = model(input_data)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(output, target_labels)\n",
    "    \n",
    "    # Perform a backward pass (compute gradients)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model's parameters using the optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss for monitoring the training progress\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([[0.4025],\n",
      "        [0.4386]])\n"
     ]
    }
   ],
   "source": [
    "# After training, you can use the trained model for predictions\n",
    "test_data = torch.tensor([[7.0, 8.0], [9.0, 10.0]], requires_grad=False)\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    predictions = model(test_data)\n",
    "    print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
