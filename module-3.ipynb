{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Training Models on Structured Data Using PyTorch\n",
    "\n",
    "## What is Structured Data?\n",
    "Structured data refers to any data that resides in a fixed field within a file or record. This includes data contained in relational databases (such as tables and SQL queries), as well as Excel spreadsheets. Structured data is typically tabular, with columns representing different attributes and rows representing instances.\n",
    "\n",
    "## Why Use Neural Networks for Structured Data?\n",
    "While structured data has traditionally been handled with classical machine learning algorithms (like linear regression, decision trees, and SVMs), neural networks can provide significant benefits:\n",
    "\n",
    "- Automatic Feature Learning: Neural networks can automatically learn to represent raw features in a way that is useful for a given task, reducing the need for manual feature engineering.\n",
    "- Scalability: Neural networks are highly scalable and can handle large and high-dimensional datasets efficiently.\n",
    "- Flexibility and Customization: Neural networks can be designed to suit various complex tasks and are not restricted to specific data distributions.\n",
    "\n",
    "## Goals of This Notebook\n",
    "\n",
    "1. Understand how to load and preprocess structured data.\n",
    "2. Learn how to build a simple neural network using PyTorch.\n",
    "3. Train this network on structured data.\n",
    "4. Evaluate the model’s performance and fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Neural Network Architecture\n",
    "\n",
    "Let's assume we are working with a dataset having 10 features (input size), and we are solving a binary classification problem (output size of 1). Here’s how we can define a simple neural network with an input layer, output layer, and hidden layer of size 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (fc3): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)  # Input layer\n",
    "        self.fc2 = nn.Linear(5, 5)   # Hidden layer\n",
    "        self.fc3 = nn.Linear(5, 1)   # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Activation function for input layer     \n",
    "        x = F.relu(self.fc2(x)) # Activation function for hidden layer\n",
    "        x = torch.sigmoid(self.fc3(x))  # Activation function for output layer\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Toy Structured Dataset\n",
    "\n",
    "For demonstration purposes, let’s create a synthetic dataset using scikit-learn's `make_classification` function. This function allows us to create a multi-class or binary classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_CLASSES=2\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=N_CLASSES, n_informative=9, \n",
    "    n_redundant=0, n_features=10, n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1,1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "- `make_classification` is used to generate a synthetic binary classification dataset with 100 samples, 10 features, and 2 classes (0 or 1).\n",
    "- `train_test_split` is used to split the dataset into training and test sets.\n",
    "We then convert the Numpy arrays to PyTorch tensors using torch.tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer Selection\n",
    "For our binary classification task, we will use the Binary Cross Entropy Loss. For optimization, we will use the popular Stochastic Gradient Descent (SGD) optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Training a neural network essentially consists of iterating through our dataset multiple times (epochs), and in each pass:\n",
    "\n",
    "Making predictions (forward pass),\n",
    "Calculating the loss (cost function),\n",
    "Computing the gradients (backward pass),\n",
    "Updating the weights (optimizer step).\n",
    "Here’s the Python code that implements these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 0.7101\n",
      "Epoch [10/100], Loss: 0.7049\n",
      "Epoch [20/100], Loss: 0.7000\n",
      "Epoch [30/100], Loss: 0.6953\n",
      "Epoch [40/100], Loss: 0.6907\n",
      "Epoch [50/100], Loss: 0.6863\n",
      "Epoch [60/100], Loss: 0.6819\n",
      "Epoch [70/100], Loss: 0.6776\n",
      "Epoch [80/100], Loss: 0.6733\n",
      "Epoch [90/100], Loss: 0.6690\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "n_epochs = 100\n",
    "\n",
    "# Store the loss at each epoch\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(x_train)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch, n_epochs, loss.item()))\n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this loop:\n",
    "\n",
    "- Forward Pass: We start by running our data through the model (outputs = model(X_train_tensor)). This is the forward pass.\n",
    "- Compute Loss: We then compute the loss/cost using our loss function (loss = criterion(outputs, y_train_tensor)).\n",
    "- Backward Pass: This step computes the gradient of the loss with respect to each parameter (loss.backward()).\n",
    "- Update the Weights: Finally, we update the weights using the gradients with the optimizer step (optimizer.step())."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "After training the model, we need to evaluate it using the test set. We don’t compute gradients here, since we are not updating the weights during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6529\n",
      "Test Accuracy: 66.00%\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# No gradient computation\n",
    "with torch.no_grad():\n",
    "    # Forward pass\n",
    "    test_outputs = model(x_test)\n",
    "    \n",
    "    # Compute the loss\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "test_outputs_np = test_outputs.numpy()\n",
    "y_test_np = y_test.numpy()\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "predicted = (test_outputs_np > 0.5).astype(int)\n",
    "accuracy = (predicted == y_test_np).mean()\n",
    "    \n",
    "print('Test Loss: {:.4f}'.format(test_loss.item()))\n",
    "print('Test Accuracy: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this evaluation phase:\n",
    "\n",
    "- We set the model to evaluation mode (model.eval()), which is essential when the network has layers like dropout or batch normalization.\n",
    "- `with torch.no_grad():` context manager is used to ensure that no gradients are computed, which reduces memory usage and speeds up computation.\n",
    "- The model’s performance is assessed using the loss calculated from the test set and the accuracy metric, which is simply the proportion of the test set that was correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Hyperparameter tuning is the process of systematically searching for the best combination of hyperparameters that optimize a model's performance on a particular task. Common hyperparameters in neural network training include the learning rate, number of epochs, batch size, and the architecture of the network itself (e.g., number of layers, number of units in each layer).\n",
    "\n",
    "### Using Grid Search\n",
    "One common approach for hyperparameter tuning is grid search. In a grid search, we specify a set of possible values for each hyperparameter we want to tune, and then we train a new model for every possible combination of these hyperparameters.\n",
    "\n",
    "Below is a simple example of hyperparameter tuning via grid search, where we try different learning rates and batch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.100, batch_size: 16, Test Loss: 0.2363, Accuracy: 89.00%\n",
      "lr: 0.010, batch_size: 16, Test Loss: 0.3046, Accuracy: 84.00%\n",
      "lr: 0.001, batch_size: 16, Test Loss: 0.5044, Accuracy: 82.00%\n",
      "lr: 0.100, batch_size: 32, Test Loss: 0.2931, Accuracy: 86.00%\n",
      "lr: 0.010, batch_size: 32, Test Loss: 0.3672, Accuracy: 85.00%\n",
      "lr: 0.001, batch_size: 32, Test Loss: 0.6239, Accuracy: 50.00%\n",
      "lr: 0.100, batch_size: 64, Test Loss: 0.2078, Accuracy: 94.00%\n",
      "lr: 0.010, batch_size: 64, Test Loss: 0.4551, Accuracy: 86.00%\n",
      "lr: 0.001, batch_size: 64, Test Loss: 0.6167, Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Define the hyperparameters\n",
    "param_grid = {\n",
    "    'lr': [0.1, 0.01, 0.001],\n",
    "    'batch_size': [16, 32, 64],\n",
    "}\n",
    "\n",
    "# Generate combinations of hyperparameters\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Store the results\n",
    "results = []\n",
    "\n",
    "# Grid search\n",
    "for params in grid:\n",
    "    # Set the hyperparameters\n",
    "    lr = params['lr']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    # Create a new model\n",
    "    model = SimpleNN()\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Data loader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(x_train, y_train), \n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training loop (for simplicity, we'll use a fixed number of epochs)\n",
    "    # Note that this will heavily affect which learning rate does best\n",
    "    for epoch in range(100):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(x_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "        \n",
    "    # Convert to numpy arrays for calculating accuracy\n",
    "    test_outputs_np = test_outputs.numpy()\n",
    "    y_test_np = y_test.numpy()\n",
    "    predicted = (test_outputs_np > 0.5).astype(int)\n",
    "    accuracy = (predicted == y_test_np).mean()\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'test_loss': test_loss.item(),\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print('lr: {:.3f}, batch_size: {:d}, Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "        result['lr'], result['batch_size'], result['test_loss'], result['accuracy'] * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see how different combinations of hyperparameters performed across 100 epochs! However, it's important to note that the lower the learning rate - the larger the number of epochs the model takes to learn - so we can't yet say that one learning rate is better than another. This is simply meant to serve as an example for how we can train deep learning models whilst also finding the best parameters for our data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
